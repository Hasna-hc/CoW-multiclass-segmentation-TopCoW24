{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "conda update -n base conda &&\n",
    "conda install -n base conda-libmamba-solver &&\n",
    "conda config --set solver libmamba &&\n",
    "source ~/anaconda3/bin/activate\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "conda create -n nnunet_topcow24 python==3.9.13 anaconda -y\n",
    "conda activate nnunet_topcow24\n",
    "conda update -n nnunet_topcow24 conda -y\n",
    "\n",
    "### MODIFIED THIS FILE : /home/hasna/miniconda3/envs/nnunet_topcow24/lib/python3.9/site-packages/acvl_utils/cropping_and_padding/bounding_boxes.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# pip install -r requirements.txt\n",
    "pip install -r /home/hasna/nnUNet_dir/requirements.txt\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "pip install threadpoolctl==3.1.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "export PYTHONPATH=\"${PYTHONPATH}:/home/hasna/miccai24_challenges/epvs_challenge/nnunet_dir\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Preparing nnUNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Installation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Following instructions from:\n",
    "https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/installation_instructions.md\n",
    "pip install -e . \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Dataset format*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob \n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "from collections import OrderedDict\n",
    "from scipy.ndimage import find_objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1 - CoW Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:52<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "''' Binary MRA segmentation using MRA only '''\n",
    "\n",
    "dataset_name = 'Dataset802_TopCoWSegBinMRA'\n",
    "nnunet_basedir = \"/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir\"\n",
    "dataset_folder_name = os.path.join(nnunet_basedir, 'dataset/nnUNet_raw/', dataset_name)\n",
    "train_image_dir = os.path.join(dataset_folder_name,'imagesTr')\n",
    "train_label_dir = os.path.join(dataset_folder_name,'labelsTr')\n",
    "test_dir = os.path.join(dataset_folder_name,'imagesTs')\n",
    "### ----------------------------------------------------------------------------------------\n",
    "\n",
    "os.makedirs(train_image_dir, exist_ok = True)\n",
    "os.makedirs(train_label_dir, exist_ok = True)\n",
    "os.makedirs(test_dir, exist_ok = True)\n",
    "### ----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "### Renaming according to nnUNet: \n",
    "data_path = f'/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir/dataset/nnUNet_raw/{dataset_name}'\n",
    "data_folder = '/home/hasna/datasets/TopCoW2024_Data_Release'\n",
    "\n",
    "subfold = ['imagesTr', 'labelsTr']\n",
    "for sub in tqdm(natsorted(os.listdir(os.path.join(data_folder, 'cow_seg_labelsTr')))):\n",
    "    if sub.startswith('topcow_mr_'):                \n",
    "        init_name = os.path.join(data_folder, 'imagesTr', f\"{sub.split('.')[0]}_0000.nii.gz\")\n",
    "        new_name = os.path.join(data_path, subfold[0], f\"{sub.split('.')[0]}_0000.nii.gz\")\n",
    "        # # shutil.copy(init_name, new_name)\n",
    "\n",
    "        init_name = os.path.join(data_folder, 'cow_seg_labelsTr', sub)\n",
    "        new_name = os.path.join(data_path, subfold[1], sub)\n",
    "        img = nib.load(init_name)\n",
    "        data = img.get_fdata()\n",
    "        data = 1*(data>0)\n",
    "        data = data.astype(np.uint8)\n",
    "        new_img = nib.Nifti1Image(data, img.affine, img.header)\n",
    "        # # nib.save(new_img, new_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:29<00:00,  2.78it/s]\n"
     ]
    }
   ],
   "source": [
    "''' Binary CTA segmentation using CTA only '''\n",
    "\n",
    "dataset_name = 'Dataset809_TopCoWSegBinCTA'\n",
    "nnunet_basedir = \"/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir\"\n",
    "dataset_folder_name = os.path.join(nnunet_basedir, 'dataset/nnUNet_raw/', dataset_name)\n",
    "train_image_dir = os.path.join(dataset_folder_name,'imagesTr')\n",
    "train_label_dir = os.path.join(dataset_folder_name,'labelsTr')\n",
    "test_dir = os.path.join(dataset_folder_name,'imagesTs')\n",
    "### ----------------------------------------------------------------------------------------\n",
    "\n",
    "os.makedirs(train_image_dir, exist_ok = True)\n",
    "os.makedirs(train_label_dir, exist_ok = True)\n",
    "os.makedirs(test_dir, exist_ok = True)\n",
    "### ----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "### Renaming according to nnUNet: \n",
    "data_path = f'/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir/dataset/nnUNet_raw/{dataset_name}'\n",
    "data_folder = '/home/hasna/datasets/TopCoW2024_Data_Release'\n",
    "\n",
    "subfold = ['imagesTr', 'labelsTr']\n",
    "for sub in tqdm(natsorted(os.listdir(os.path.join(data_folder, 'cow_seg_labelsTr')))):\n",
    "    if sub.startswith('topcow_ct_'):                \n",
    "        init_name = os.path.join(data_folder, 'imagesTr', f\"{sub.split('.')[0]}_0000.nii.gz\")\n",
    "        new_name = os.path.join(data_path, subfold[0], f\"{sub.split('.')[0]}_0000.nii.gz\")\n",
    "        # # shutil.copy(init_name, new_name)\n",
    "\n",
    "        init_name = os.path.join(data_folder, 'cow_seg_labelsTr', sub)\n",
    "        new_name = os.path.join(data_path, subfold[1], sub)\n",
    "        img = nib.load(init_name)\n",
    "        data = img.get_fdata()\n",
    "        data = 1*(data>0)\n",
    "        data = data.astype(np.uint8)\n",
    "        new_img = nib.Nifti1Image(data, img.affine, img.header)\n",
    "        # # nib.save(new_img, new_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [05:33<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "''' CTA and MRA for multiclass seg in the final version for CTA '''\n",
    "\n",
    "dataset_name = 'Dataset806_TopCoWSegCTAMRA'\n",
    "nnunet_basedir = \"/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir\"\n",
    "dataset_folder_name = os.path.join(nnunet_basedir, 'dataset/nnUNet_raw/', dataset_name)\n",
    "train_image_dir = os.path.join(dataset_folder_name,'imagesTr')\n",
    "train_label_dir = os.path.join(dataset_folder_name,'labelsTr')\n",
    "test_dir = os.path.join(dataset_folder_name,'imagesTs')\n",
    "### ----------------------------------------------------------------------------------------\n",
    "\n",
    "os.makedirs(train_image_dir, exist_ok = True)\n",
    "os.makedirs(train_label_dir, exist_ok = True)\n",
    "os.makedirs(test_dir, exist_ok = True)\n",
    "### ----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "### Renaming according to nnUNet: \n",
    "data_path = f'/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir/dataset/nnUNet_raw/{dataset_name}'\n",
    "data_folder = '/home/hasna/datasets/TopCoW2024_Data_Release'\n",
    "\n",
    "subfold = ['imagesTr', 'labelsTr']\n",
    "for sub in tqdm(natsorted(os.listdir(os.path.join(data_folder, 'cow_seg_labelsTr')))):\n",
    "    if sub.endswith('.nii.gz'):                \n",
    "        init_name = os.path.join(data_folder, 'imagesTr', f\"{sub.split('.')[0]}_0000.nii.gz\")\n",
    "        new_name = os.path.join(data_path, subfold[0], f\"{sub.split('.')[0]}_0000.nii.gz\")\n",
    "        # # shutil.copy(init_name, new_name)\n",
    "\n",
    "        init_name = os.path.join(data_folder, 'cow_seg_labelsTr', sub)\n",
    "        new_name = os.path.join(data_path, subfold[1], sub)\n",
    "        img = nib.load(init_name)\n",
    "        data = img.get_fdata()\n",
    "        data[data==15] = 13\n",
    "        data = data.astype(np.uint8)\n",
    "        new_img = nib.Nifti1Image(data, img.affine, img.header)\n",
    "        # # nib.save(new_img, new_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:07<00:00,  3.69it/s] \n"
     ]
    }
   ],
   "source": [
    "''' Multiclass MRA only '''\n",
    "\n",
    "dataset_name = 'Dataset808_TopCoWSegMRA'\n",
    "nnunet_basedir = \"/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir\"\n",
    "dataset_folder_name = os.path.join(nnunet_basedir, 'dataset/nnUNet_raw/', dataset_name)\n",
    "train_image_dir = os.path.join(dataset_folder_name,'imagesTr')\n",
    "train_label_dir = os.path.join(dataset_folder_name,'labelsTr')\n",
    "test_dir = os.path.join(dataset_folder_name,'imagesTs')\n",
    "### ----------------------------------------------------------------------------------------\n",
    "\n",
    "os.makedirs(train_image_dir, exist_ok = True)\n",
    "os.makedirs(train_label_dir, exist_ok = True)\n",
    "os.makedirs(test_dir, exist_ok = True)\n",
    "### ----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "### Renaming according to nnUNet: \n",
    "data_path = f'/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir/dataset/nnUNet_raw/{dataset_name}'\n",
    "data_folder = '/home/hasna/datasets/TopCoW2024_Data_Release'\n",
    "\n",
    "subfold = ['imagesTr', 'labelsTr']\n",
    "for sub in tqdm(natsorted(os.listdir(os.path.join(data_folder, 'cow_seg_labelsTr')))):\n",
    "    if sub.startswith('topcow_mr_'):                \n",
    "        init_name = os.path.join(data_folder, 'imagesTr', f\"{sub.split('.')[0]}_0000.nii.gz\")\n",
    "        new_name = os.path.join(data_path, subfold[0], f\"{sub.split('.')[0]}_0000.nii.gz\")\n",
    "        # # shutil.copy(init_name, new_name)\n",
    "\n",
    "        init_name = os.path.join(data_folder, 'cow_seg_labelsTr', sub)\n",
    "        new_name = os.path.join(data_path, subfold[1], sub)\n",
    "        img = nib.load(init_name)\n",
    "        data = img.get_fdata()\n",
    "        data[data==15] = 13\n",
    "        data = data.astype(np.uint8)\n",
    "        new_img = nib.Nifti1Image(data, img.affine, img.header)\n",
    "        # # nib.save(new_img, new_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Preparing dataset.json file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Task 1: CoW Segmentation '''\n",
    "### Binary CTA\n",
    "\n",
    "dataset_name = 'Dataset809_TopCoWSegBinCTA'\n",
    "nnunet_basedir = \"/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir\"\n",
    "dataset_folder_name = os.path.join(nnunet_basedir, 'dataset/nnUNet_raw/', dataset_name)\n",
    "train_image_dir = os.path.join(dataset_folder_name,'imagesTr')\n",
    "train_label_dir = os.path.join(dataset_folder_name,'labelsTr')\n",
    "test_dir = os.path.join(dataset_folder_name,'imagesTs')\n",
    "\n",
    "dataset_json_path = os.path.join(dataset_folder_name, 'dataset.json')\n",
    "overwrite_json_file = True #make it True if you want to overwrite the dataset.json file in Task_folder\n",
    "json_file_exist = False\n",
    "\n",
    "if os.path.exists(dataset_json_path):\n",
    "    print('dataset.json already exist!')\n",
    "    json_file_exist = True\n",
    "\n",
    "if json_file_exist==False or overwrite_json_file:\n",
    "    json_dict = OrderedDict()\n",
    "    json_dict['dataset_name'] = dataset_name\n",
    "    json_dict['description'] = \"Binary CoW segmentation task for CTA using CTA only, trained for all folds for the TopCoW2024 Challenge\" #\"Binary CoW roi Detection task using CTA and Binary segmentation mask from the multiclass GT, trained for 5 folds for the TopCoW2024 Challenge\"\n",
    "    json_dict['tensor_image_size'] = \"3D\"\n",
    "    json_dict['file_ending'] = \".nii.gz\"\n",
    "\n",
    "    #you may mention more than one modality\n",
    "    json_dict['channel_names'] = {\n",
    "        \"0\": \"cta\",\n",
    "    }\n",
    "    #labels+1 should be mentioned for all the labels in the dataset\n",
    "    json_dict['labels'] = {\n",
    "        \"background\": 0,\n",
    "        \"cow\" : 1        \n",
    "    }\n",
    "\n",
    "    train_ids = sorted(os.listdir(train_label_dir))\n",
    "    test_ids = sorted(os.listdir(test_dir))\n",
    "    json_dict['numTraining'] = len(train_ids)\n",
    "    json_dict['numTesting'] = len(test_ids)\n",
    "\n",
    "    # no modality in train image and labels in dataset.json \n",
    "    json_dict['training'] = [{'image': \"./imagesTr/%s\" % i, \"label\": \"./labelsTr/%s\" % i} for i in train_ids]\n",
    "\n",
    "    # removing the modality from test image name to be saved in dataset.json\n",
    "    json_dict['test'] = [\"./imagesTs/%s\" % i for i in test_ids]\n",
    "\n",
    "    with open(dataset_json_path, 'w') as f:\n",
    "        json.dump(json_dict, f, indent=4, sort_keys=True)\n",
    "\n",
    "    if os.path.exists(dataset_json_path):\n",
    "        if json_file_exist==False:\n",
    "            print('dataset.json created!')\n",
    "        else: \n",
    "            print('dataset.json overwritten!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.json created!\n"
     ]
    }
   ],
   "source": [
    "''' Task 1: CoW Segmentation '''\n",
    "### Multiclass CTAMRA for CTA\n",
    "\n",
    "dataset_name = 'Dataset806_TopCoWSegCTAMRA'\n",
    "nnunet_basedir = \"/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir\"\n",
    "dataset_folder_name = os.path.join(nnunet_basedir, 'dataset/nnUNet_raw/', dataset_name)\n",
    "train_image_dir = os.path.join(dataset_folder_name,'imagesTr')\n",
    "train_label_dir = os.path.join(dataset_folder_name,'labelsTr')\n",
    "test_dir = os.path.join(dataset_folder_name,'imagesTs')\n",
    "\n",
    "dataset_json_path = os.path.join(dataset_folder_name, 'dataset.json')\n",
    "overwrite_json_file = True #make it True if you want to overwrite the dataset.json file in Task_folder\n",
    "json_file_exist = False\n",
    "\n",
    "if os.path.exists(dataset_json_path):\n",
    "    print('dataset.json already exist!')\n",
    "    json_file_exist = True\n",
    "\n",
    "if json_file_exist==False or overwrite_json_file:\n",
    "    json_dict = OrderedDict()\n",
    "    json_dict['dataset_name'] = dataset_name\n",
    "    json_dict['description'] = \"Multiclass CoW segmentation task for CTA using CTA+MRA, trained for all folds for the TopCoW2024 Challenge\" #\"Binary CoW roi Detection task using CTA and Binary segmentation mask from the multiclass GT, trained for 5 folds for the TopCoW2024 Challenge\"\n",
    "    json_dict['tensor_image_size'] = \"3D\"\n",
    "    json_dict['file_ending'] = \".nii.gz\"\n",
    "\n",
    "    #you may mention more than one modality\n",
    "    json_dict['channel_names'] = {\n",
    "        \"0\": \"ctamra\",\n",
    "    }\n",
    "    #labels+1 should be mentioned for all the labels in the dataset\n",
    "    json_dict['labels'] = {\n",
    "        \"background\": 0,\n",
    "        \"BA\" : 1,\n",
    "        \"R-PCA\" : 2,\n",
    "        \"L-PCA\" : 3,\n",
    "        \"R-ICA\" : 4,\n",
    "        \"R-MCA\" : 5,\n",
    "        \"L-ICA\" : 6,\n",
    "        \"L-MCA\" : 7,\n",
    "        \"R-Pcom\" : 8,\n",
    "        \"L-Pcom\" : 9,\n",
    "        \"Acom\" : 10,\n",
    "        \"R-ACA\" : 11,\n",
    "        \"L-ACA\" : 12,\n",
    "        \"3rd-A2\" : 13\n",
    "    }\n",
    "\n",
    "    train_ids = sorted(os.listdir(train_label_dir))\n",
    "    test_ids = sorted(os.listdir(test_dir))\n",
    "    json_dict['numTraining'] = len(train_ids)\n",
    "    json_dict['numTesting'] = len(test_ids)\n",
    "\n",
    "    # no modality in train image and labels in dataset.json \n",
    "    json_dict['training'] = [{'image': \"./imagesTr/%s\" % i, \"label\": \"./labelsTr/%s\" % i} for i in train_ids]\n",
    "\n",
    "    # removing the modality from test image name to be saved in dataset.json\n",
    "    json_dict['test'] = [\"./imagesTs/%s\" % i for i in test_ids]\n",
    "\n",
    "    with open(dataset_json_path, 'w') as f:\n",
    "        json.dump(json_dict, f, indent=4, sort_keys=True)\n",
    "\n",
    "    if os.path.exists(dataset_json_path):\n",
    "        if json_file_exist==False:\n",
    "            print('dataset.json created!')\n",
    "        else: \n",
    "            print('dataset.json overwritten!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Task 1: CoW Segmentation '''\n",
    "### Binary MRA\n",
    "\n",
    "dataset_name = 'Dataset802_TopCoWSegBinMRA'\n",
    "nnunet_basedir = \"/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir\"\n",
    "dataset_folder_name = os.path.join(nnunet_basedir, 'dataset/nnUNet_raw/', dataset_name)\n",
    "train_image_dir = os.path.join(dataset_folder_name,'imagesTr')\n",
    "train_label_dir = os.path.join(dataset_folder_name,'labelsTr')\n",
    "test_dir = os.path.join(dataset_folder_name,'imagesTs')\n",
    "\n",
    "dataset_json_path = os.path.join(dataset_folder_name, 'dataset.json')\n",
    "overwrite_json_file = True #make it True if you want to overwrite the dataset.json file in Task_folder\n",
    "json_file_exist = False\n",
    "\n",
    "if os.path.exists(dataset_json_path):\n",
    "    print('dataset.json already exist!')\n",
    "    json_file_exist = True\n",
    "\n",
    "if json_file_exist==False or overwrite_json_file:\n",
    "    json_dict = OrderedDict()\n",
    "    json_dict['dataset_name'] = dataset_name\n",
    "    json_dict['description'] = \"Binary CoW segmentation task for MRA using MRA only, trained for all folds for the TopCoW2024 Challenge\" #\"Binary CoW roi Detection task using CTA and Binary segmentation mask from the multiclass GT, trained for 5 folds for the TopCoW2024 Challenge\"\n",
    "    json_dict['tensor_image_size'] = \"3D\"\n",
    "    json_dict['file_ending'] = \".nii.gz\"\n",
    "\n",
    "    #you may mention more than one modality\n",
    "    json_dict['channel_names'] = {\n",
    "        \"0\": \"mra\",\n",
    "    }\n",
    "    #labels+1 should be mentioned for all the labels in the dataset\n",
    "    json_dict['labels'] = {\n",
    "        \"background\": 0,\n",
    "        \"cow\" : 1        \n",
    "    }\n",
    "\n",
    "    train_ids = sorted(os.listdir(train_label_dir))\n",
    "    test_ids = sorted(os.listdir(test_dir))\n",
    "    json_dict['numTraining'] = len(train_ids)\n",
    "    json_dict['numTesting'] = len(test_ids)\n",
    "\n",
    "    # no modality in train image and labels in dataset.json \n",
    "    json_dict['training'] = [{'image': \"./imagesTr/%s\" % i, \"label\": \"./labelsTr/%s\" % i} for i in train_ids]\n",
    "\n",
    "    # removing the modality from test image name to be saved in dataset.json\n",
    "    json_dict['test'] = [\"./imagesTs/%s\" % i for i in test_ids]\n",
    "\n",
    "    with open(dataset_json_path, 'w') as f:\n",
    "        json.dump(json_dict, f, indent=4, sort_keys=True)\n",
    "\n",
    "    if os.path.exists(dataset_json_path):\n",
    "        if json_file_exist==False:\n",
    "            print('dataset.json created!')\n",
    "        else: \n",
    "            print('dataset.json overwritten!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.json created!\n"
     ]
    }
   ],
   "source": [
    "''' Task 1: CoW Segmentation '''\n",
    "### Multiclass MRA\n",
    "\n",
    "dataset_name = 'Dataset808_TopCoWSegMRA'\n",
    "nnunet_basedir = \"/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir\"\n",
    "dataset_folder_name = os.path.join(nnunet_basedir, 'dataset/nnUNet_raw/', dataset_name)\n",
    "train_image_dir = os.path.join(dataset_folder_name,'imagesTr')\n",
    "train_label_dir = os.path.join(dataset_folder_name,'labelsTr')\n",
    "test_dir = os.path.join(dataset_folder_name,'imagesTs')\n",
    "\n",
    "dataset_json_path = os.path.join(dataset_folder_name, 'dataset.json')\n",
    "overwrite_json_file = True #make it True if you want to overwrite the dataset.json file in Task_folder\n",
    "json_file_exist = False\n",
    "\n",
    "if os.path.exists(dataset_json_path):\n",
    "    print('dataset.json already exist!')\n",
    "    json_file_exist = True\n",
    "\n",
    "if json_file_exist==False or overwrite_json_file:\n",
    "    json_dict = OrderedDict()\n",
    "    json_dict['dataset_name'] = dataset_name\n",
    "    json_dict['description'] = \"Multiclass CoW segmentation task for MRA, trained on 5 folds for the TopCoW2024 Challenge\" #\"Binary CoW roi Detection task using CTA and Binary segmentation mask from the multiclass GT, trained for 5 folds for the TopCoW2024 Challenge\"\n",
    "    json_dict['tensor_image_size'] = \"3D\"\n",
    "    json_dict['file_ending'] = \".nii.gz\"\n",
    "\n",
    "    #you may mention more than one modality\n",
    "    json_dict['channel_names'] = {\n",
    "        \"0\": \"mra\",\n",
    "    }\n",
    "    #labels+1 should be mentioned for all the labels in the dataset\n",
    "    json_dict['labels'] = {\n",
    "        \"background\": 0,\n",
    "        \"BA\" : 1,\n",
    "        \"R-PCA\" : 2,\n",
    "        \"L-PCA\" : 3,\n",
    "        \"R-ICA\" : 4,\n",
    "        \"R-MCA\" : 5,\n",
    "        \"L-ICA\" : 6,\n",
    "        \"L-MCA\" : 7,\n",
    "        \"R-Pcom\" : 8,\n",
    "        \"L-Pcom\" : 9,\n",
    "        \"Acom\" : 10,\n",
    "        \"R-ACA\" : 11,\n",
    "        \"L-ACA\" : 12,\n",
    "        \"3rd-A2\" : 13\n",
    "    }\n",
    "\n",
    "    train_ids = sorted(os.listdir(train_label_dir))\n",
    "    test_ids = sorted(os.listdir(test_dir))\n",
    "    json_dict['numTraining'] = len(train_ids)\n",
    "    json_dict['numTesting'] = len(test_ids)\n",
    "\n",
    "    # no modality in train image and labels in dataset.json \n",
    "    json_dict['training'] = [{'image': \"./imagesTr/%s\" % i, \"label\": \"./labelsTr/%s\" % i} for i in train_ids]\n",
    "\n",
    "    # removing the modality from test image name to be saved in dataset.json\n",
    "    json_dict['test'] = [\"./imagesTs/%s\" % i for i in test_ids]\n",
    "\n",
    "    with open(dataset_json_path, 'w') as f:\n",
    "        json.dump(json_dict, f, indent=4, sort_keys=True)\n",
    "\n",
    "    if os.path.exists(dataset_json_path):\n",
    "        if json_file_exist==False:\n",
    "            print('dataset.json created!')\n",
    "        else: \n",
    "            print('dataset.json overwritten!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:22<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "''' Information about Spacings and Sizes of all modalities '''\n",
    "\n",
    "dataset_name = 'Dataset701_TopCoWDetCTA' \n",
    "# Renaming according to nnUNet: \n",
    "data_folder = '/home/hasna/datasets/TopCoW2024_Data_Release'\n",
    "subfold = ['imagesTr', 'cow_seg_labelsTr']\n",
    "\n",
    "mri_spacing, seg_spacing = [], []\n",
    "mri_size, seg_size = [], []\n",
    "labels = []\n",
    "fnames = []\n",
    "\n",
    "# imageTr:\n",
    "for sub in tqdm(natsorted(os.listdir(os.path.join(data_folder, 'cow_seg_labelsTr')))):\n",
    "    if sub.endswith('.nii.gz'):        \n",
    "        mri = sitk.ReadImage(os.path.join(data_folder, subfold[0], f\"{sub.split('.')[0]}_0000.nii.gz\"))\n",
    "        mri_spacing.append(mri.GetSpacing())\n",
    "        mri_size.append(mri.GetSize())\n",
    "        \n",
    "        seg = sitk.ReadImage(os.path.join(data_folder, subfold[1], sub))\n",
    "        seg_spacing.append(seg.GetSpacing())\n",
    "        seg_size.append(seg.GetSize())\n",
    "\n",
    "        fnames.append(sub.split('.')[0])\n",
    "        labels.append(sitk.GetArrayFromImage(seg).max())\n",
    "\n",
    "df = pd.DataFrame({'filename':fnames, 'mri_spacing':mri_spacing, 'seg_spacing':seg_spacing, 'mri_size':mri_size, 'seg_size':seg_size, 'max_label':labels})\n",
    "# df.to_csv('/home/hasna/miccai24_challenges/topcow_challenge/topcow24_dataset_spacing_size.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:18<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "''' Information about Spacings and Sizes of all modalities '''\n",
    "\n",
    "data_folder = '/home/hasna/datasets/crown_20_mr_01062023'\n",
    "\n",
    "mri_spacing, seg_spacing = [], []\n",
    "mri_size, seg_size = [], []\n",
    "labels = []\n",
    "fnames = []\n",
    "\n",
    "# imageTr:\n",
    "for sub in tqdm(natsorted(os.listdir(os.path.join(data_folder, 'mul_labelsTr')))):\n",
    "    if sub.startswith('crown_mr_whole'):        \n",
    "        mri = sitk.ReadImage(os.path.join(data_folder, 'imagesTr', f\"{sub.split('.')[0]}_0000.nii.gz\"))\n",
    "        mri_spacing.append(mri.GetSpacing())\n",
    "        mri_size.append(mri.GetSize())\n",
    "        \n",
    "        seg = sitk.ReadImage(os.path.join(data_folder, 'mul_labelsTr', sub))\n",
    "        seg_spacing.append(seg.GetSpacing())\n",
    "        seg_size.append(seg.GetSize())\n",
    "\n",
    "        fnames.append(sub.split('.')[0])\n",
    "        labels.append(sitk.GetArrayFromImage(seg).max())\n",
    "\n",
    "df = pd.DataFrame({'filename':fnames, 'mri_spacing':mri_spacing, 'seg_spacing':seg_spacing, 'mri_size':mri_size, 'seg_size':seg_size, 'max_label':labels})\n",
    "# df.to_csv('/home/hasna/miccai24_challenges/topcow_challenge/crown23_dataset_spacing_size.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Training nnUNet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nexport nnUNet_raw='/home/hasna/miccai24_challenges/topcow_challenge_final/nnunet_dir/dataset/nnUNet_raw'\\nexport nnUNet_preprocessed='/home/hasna/miccai24_challenges/topcow_challenge_final/nnunet_dir/dataset/preprocessed'\\nexport nnUNet_results='/home/hasna/miccai24_challenges/topcow_challenge_final/nnunet_dir/datasetnnUNet_trained_models'\\n\\n\\n>>> For task 1:\\nnnUNetv2_extract_fingerprint -d 802 --verify_dataset_integrity   # --> to get the 'dataset_fingerprint.json'\\nnnUNetv2_plan_experiment -d 802 --verify_dataset_integrity   # --> to get the 'dataset.json' and 'nnUNetPlans.json'\\nnnUNetv2_preprocess -d 802 -c 3d_fullres -np 8 --verify_dataset_integrity   # --> to get the 'gt_segmentations' and 'nnUNetPlans_3d_fullres' ...\\n\\n\\nnnUNetv2_extract_fingerprint -d 806 --verify_dataset_integrity   # --> to get the 'dataset_fingerprint.json'\\nnnUNetv2_plan_experiment -d 806 --verify_dataset_integrity   # --> to get the 'dataset.json' and 'nnUNetPlans.json'\\nnnUNetv2_preprocess -d 806 -c 3d_fullres_ps -np 8 --verify_dataset_integrity   # --> to get the 'gt_segmentations' and 'nnUNetPlans_3d_fullres' ...\\n\\n\\nnnUNetv2_extract_fingerprint -d 807 --verify_dataset_integrity   # --> to get the 'dataset_fingerprint.json'\\nnnUNetv2_plan_experiment -d 807 --verify_dataset_integrity   # --> to get the 'dataset.json' and 'nnUNetPlans.json'\\nnnUNetv2_preprocess -d 807 -c 3d_fullres_ps -np 8 --verify_dataset_integrity   # --> to get the 'gt_segmentations' and 'nnUNetPlans_3d_fullres' ...\\n\\n\\nnnUNetv2_extract_fingerprint -d 808 --verify_dataset_integrity   # --> to get the 'dataset_fingerprint.json'\\nnnUNetv2_plan_experiment -d 808 --verify_dataset_integrity   # --> to get the 'dataset.json' and 'nnUNetPlans.json'\\nnnUNetv2_preprocess -d 808 -c 3d_fullres -np 8 --verify_dataset_integrity   # --> to get the 'gt_segmentations' and 'nnUNetPlans_3d_fullres' ...\\n\\n\\n\\n>>> For task 2:\\nnnUNetv2_extract_fingerprint -d 704 --verify_dataset_integrity   # --> to get the 'dataset_fingerprint.json'\\nnnUNetv2_plan_experiment -d 704 --verify_dataset_integrity   # --> to get the 'dataset.json' and 'nnUNetPlans.json'\\nnnUNetv2_preprocess -d 704 -c 3d_fullres -np 8 --verify_dataset_integrity   # --> to get the 'gt_segmentations' and 'nnUNetPlans_3d_fullres' ...\\n\\n\\nnnUNetv2_extract_fingerprint -d 705 --verify_dataset_integrity   # --> to get the 'dataset_fingerprint.json'\\nnnUNetv2_plan_experiment -d 705 --verify_dataset_integrity   # --> to get the 'dataset.json' and 'nnUNetPlans.json'\\nnnUNetv2_preprocess -d 705 -c 3d_fullres_spacing_ps -np 8 --verify_dataset_integrity   # --> to get the 'gt_segmentations' and 'nnUNetPlans_3d_fullres' ...\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "export nnUNet_raw='/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir/dataset/nnUNet_raw'\n",
    "export nnUNet_preprocessed='/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir/dataset/preprocessed'\n",
    "export nnUNet_results='/home/hasna/miccai24_challenges/topcow_challenge/nnunet_dir/datasetnnUNet_trained_models'\n",
    "\n",
    "\n",
    ">>> For task 1:\n",
    "nnUNetv2_extract_fingerprint -d 802 --verify_dataset_integrity   # --> to get the 'dataset_fingerprint.json'\n",
    "nnUNetv2_plan_experiment -d 802 --verify_dataset_integrity   # --> to get the 'dataset.json' and 'nnUNetPlans.json'\n",
    "nnUNetv2_preprocess -d 802 -c 3d_fullres -np 8 --verify_dataset_integrity   # --> to get the 'gt_segmentations' and 'nnUNetPlans_3d_fullres' ...\n",
    "\n",
    "nnUNetv2_extract_fingerprint -d 806 --verify_dataset_integrity   # --> to get the 'dataset_fingerprint.json'\n",
    "nnUNetv2_plan_experiment -d 806 --verify_dataset_integrity   # --> to get the 'dataset.json' and 'nnUNetPlans.json'\n",
    "nnUNetv2_preprocess -d 806 -c 3d_fullres_ps -np 8 --verify_dataset_integrity   # --> to get the 'gt_segmentations' and 'nnUNetPlans_3d_fullres' ...\n",
    "\n",
    "nnUNetv2_extract_fingerprint -d 808 --verify_dataset_integrity   # --> to get the 'dataset_fingerprint.json'\n",
    "nnUNetv2_plan_experiment -d 808 --verify_dataset_integrity   # --> to get the 'dataset.json' and 'nnUNetPlans.json'\n",
    "nnUNetv2_preprocess -d 808 -c 3d_fullres -np 8 --verify_dataset_integrity   # --> to get the 'gt_segmentations' and 'nnUNetPlans_3d_fullres' ...\n",
    "\n",
    "nnUNetv2_extract_fingerprint -d 809 --verify_dataset_integrity   # --> to get the 'dataset_fingerprint.json'\n",
    "nnUNetv2_plan_experiment -d 809 --verify_dataset_integrity   # --> to get the 'dataset.json' and 'nnUNetPlans.json'\n",
    "nnUNetv2_preprocess -d 809 -c 3d_fullres -np 8 --verify_dataset_integrity   # --> to get the 'gt_segmentations' and 'nnUNetPlans_3d_fullres' ...\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' For task 1: -------------------------------------------------------------------------------------------\n",
    ">>> Training for MRA:\n",
    ">> [Binary - MRA only]:\n",
    "CUDA_VISIBLE_DEVICES=0 nnUNetv2_train 802 3d_fullres all -tr nnUNetTrainerSkeletonRecall\n",
    "\n",
    ">> [Multiclass - MRA only]:\n",
    "CUDA_VISIBLE_DEVICES=0 nnUNetv2_train 808 3d_fullres 0 -tr nnUNetTrainerSkeletonRecallBinDiceNoMirroring -pretrained_weights /home/hasna/miccai24_challenges/topcow_challenge/Dataset101_Mulseg/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/checkpoint_best.pth\n",
    "CUDA_VISIBLE_DEVICES=3 nnUNetv2_train 808 3d_fullres 1 -tr nnUNetTrainerSkeletonRecallBinDiceNoMirroring -pretrained_weights /home/hasna/miccai24_challenges/topcow_challenge/Dataset101_Mulseg/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_1/checkpoint_best.pth\n",
    "CUDA_VISIBLE_DEVICES=0 nnUNetv2_train 808 3d_fullres 2 -tr nnUNetTrainerSkeletonRecallBinDiceNoMirroring -pretrained_weights /home/hasna/miccai24_challenges/topcow_challenge/Dataset101_Mulseg/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_2/checkpoint_best.pth\n",
    "CUDA_VISIBLE_DEVICES=3 nnUNetv2_train 808 3d_fullres 3 -tr nnUNetTrainerSkeletonRecallBinDiceNoMirroring -pretrained_weights /home/hasna/miccai24_challenges/topcow_challenge/Dataset101_Mulseg/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth\n",
    "CUDA_VISIBLE_DEVICES=0 nnUNetv2_train 808 3d_fullres 4 -tr nnUNetTrainerSkeletonRecallBinDiceNoMirroring -pretrained_weights /home/hasna/miccai24_challenges/topcow_challenge/Dataset101_Mulseg/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_4/checkpoint_best.pth\n",
    "\n",
    "\n",
    ">>> Training for CTA:\n",
    ">> [Binary - CTA only]:\n",
    "CUDA_VISIBLE_DEVICES=2 nnUNetv2_train 809 3d_fullres all -tr nnUNetTrainerSkeletonRecall --val_best\n",
    "\n",
    ">> [Multiclass - CTA+MRA]\n",
    "CUDA_VISIBLE_DEVICES=3 nnUNetv2_train 806 3d_fullres_ps 0 -tr nnUNetTrainerSkeletonRecallBinDiceNoMirroring\n",
    "CUDA_VISIBLE_DEVICES=0 nnUNetv2_train 806 3d_fullres_ps 1 -tr nnUNetTrainerSkeletonRecallBinDiceNoMirroring\n",
    "CUDA_VISIBLE_DEVICES=3 nnUNetv2_train 806 3d_fullres_ps 2 -tr nnUNetTrainerSkeletonRecallBinDiceNoMirroring\n",
    "CUDA_VISIBLE_DEVICES=0 nnUNetv2_train 806 3d_fullres_ps 3 -tr nnUNetTrainerSkeletonRecallBinDiceNoMirroring\n",
    "CUDA_VISIBLE_DEVICES=1 nnUNetv2_train 806 3d_fullres_ps 4 -tr nnUNetTrainerSkeletonRecallBinDiceNoMirroring\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cd /home/hasna/miccai24_challenges/TopCoW_Algo_Submission/task-1-seg\n",
    "bash test_run.sh\n",
    "bash save.sh\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnunet_epvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
